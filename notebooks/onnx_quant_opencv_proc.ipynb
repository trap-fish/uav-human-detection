{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U991IVdm6Xc4"
      },
      "source": [
        "If you need the coco.yaml file, can copy/paste this - save to /content/coco.yaml\n",
        "\n",
        "it's just the [Ultralytics](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/coco.yaml) one but with updated datapaths and the download stuff removed (as it doesn't work easily on colab) also only the val data url since no training is needed.\n",
        "\n",
        "Val2017 is about 1gb\n",
        "\n",
        "\n",
        "```\n",
        "# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license\n",
        "\n",
        "# COCO 2017 dataset http://cocodataset.org by Microsoft\n",
        "# Example usage: python train.py --data coco.yaml\n",
        "# parent\n",
        "# â”œâ”€â”€ yolov5\n",
        "# â””â”€â”€ datasets\n",
        "#     â””â”€â”€ coco  â† downloads here (20.1 GB)\n",
        "\n",
        "# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\n",
        "path: /content/datasets/coco # dataset root dir\n",
        "train: val2017.txt # only for quantisation, don't need train but sometimes ultralyics throws error if not here\n",
        "val: val2017.txt # val images (relative to 'path') 5000 images\n",
        "\n",
        "# Classes\n",
        "names:\n",
        "  0: person\n",
        "  1: bicycle\n",
        "  2: car\n",
        "  3: motorcycle\n",
        "  4: airplane\n",
        "  5: bus\n",
        "  6: train\n",
        "  7: truck\n",
        "  8: boat\n",
        "  9: traffic light\n",
        "  10: fire hydrant\n",
        "  11: stop sign\n",
        "  12: parking meter\n",
        "  13: bench\n",
        "  14: bird\n",
        "  15: cat\n",
        "  16: dog\n",
        "  17: horse\n",
        "  18: sheep\n",
        "  19: cow\n",
        "  20: elephant\n",
        "  21: bear\n",
        "  22: zebra\n",
        "  23: giraffe\n",
        "  24: backpack\n",
        "  25: umbrella\n",
        "  26: handbag\n",
        "  27: tie\n",
        "  28: suitcase\n",
        "  29: frisbee\n",
        "  30: skis\n",
        "  31: snowboard\n",
        "  32: sports ball\n",
        "  33: kite\n",
        "  34: baseball bat\n",
        "  35: baseball glove\n",
        "  36: skateboard\n",
        "  37: surfboard\n",
        "  38: tennis racket\n",
        "  39: bottle\n",
        "  40: wine glass\n",
        "  41: cup\n",
        "  42: fork\n",
        "  43: knife\n",
        "  44: spoon\n",
        "  45: bowl\n",
        "  46: banana\n",
        "  47: apple\n",
        "  48: sandwich\n",
        "  49: orange\n",
        "  50: broccoli\n",
        "  51: carrot\n",
        "  52: hot dog\n",
        "  53: pizza\n",
        "  54: donut\n",
        "  55: cake\n",
        "  56: chair\n",
        "  57: couch\n",
        "  58: potted plant\n",
        "  59: bed\n",
        "  60: dining table\n",
        "  61: toilet\n",
        "  62: tv\n",
        "  63: laptop\n",
        "  64: mouse\n",
        "  65: remote\n",
        "  66: keyboard\n",
        "  67: cell phone\n",
        "  68: microwave\n",
        "  69: oven\n",
        "  70: toaster\n",
        "  71: sink\n",
        "  72: refrigerator\n",
        "  73: book\n",
        "  74: clock\n",
        "  75: vase\n",
        "  76: scissors\n",
        "  77: teddy bear\n",
        "  78: hair drier\n",
        "  79: toothbrush\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_7nr2dzGnYaa",
        "outputId": "d6ecb041-6006-498e-d367-df4b627b0092"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.20.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.81-py3-none-any.whl.metadata (35 kB)\n",
            "Collecting onnx2tf\n",
            "  Downloading onnx2tf-1.26.8-py3-none-any.whl.metadata (147 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m147.5/147.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnx-graphsurgeon\n",
            "  Downloading onnx_graphsurgeon-0.5.5-py2.py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting sng4onnx\n",
            "  Downloading sng4onnx-1.0.4-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting tflite_support\n",
            "  Downloading tflite_support-0.4.4-cp311-cp311-manylinux2014_x86_64.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from onnx) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from onnx) (4.25.6)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.2.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (24.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.13.1)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.20.1+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.13.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tflite_support) (1.4.0)\n",
            "Collecting protobuf>=3.20.2 (from onnx)\n",
            "  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Collecting sounddevice>=0.4.4 (from tflite_support)\n",
            "  Downloading sounddevice-0.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting pybind11>=2.6.0 (from tflite_support)\n",
            "  Downloading pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->tflite_support) (1.17.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->tflite_support) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.20.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics-8.3.81-py3-none-any.whl (922 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m922.1/922.1 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx2tf-1.26.8-py3-none-any.whl (446 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m446.2/446.2 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx_graphsurgeon-0.5.5-py2.py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sng4onnx-1.0.4-py3-none-any.whl (5.9 kB)\n",
            "Downloading tflite_support-0.4.4-cp311-cp311-manylinux2014_x86_64.whl (60.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.8/60.8 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m243.3/243.3 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sounddevice-0.5.1-py3-none-any.whl (32 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
            "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sng4onnx, pybind11, protobuf, onnx2tf, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, humanfriendly, sounddevice, onnx, nvidia-cusparse-cu12, nvidia-cudnn-cu12, coloredlogs, tflite_support, onnxruntime, onnx-graphsurgeon, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.6\n",
            "    Uninstalling protobuf-4.25.6:\n",
            "      Successfully uninstalled protobuf-4.25.6\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.16.1 requires protobuf<6.0.0dev,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\n",
            "grpcio-status 1.62.3 requires protobuf>=4.21.6, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed coloredlogs-15.0.1 humanfriendly-10.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 onnx-1.17.0 onnx-graphsurgeon-0.5.5 onnx2tf-1.26.8 onnxruntime-1.20.1 protobuf-3.20.3 pybind11-2.13.6 sng4onnx-1.0.4 sounddevice-0.5.1 tflite_support-0.4.4 ultralytics-8.3.81 ultralytics-thop-2.0.14\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "ae4ffc2c58b044d9855689ada6b69ec6"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install onnx onnxruntime ultralytics onnx2tf onnx-graphsurgeon sng4onnx tflite_support\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nztyDRYVp64F",
        "outputId": "0383c478-343a-413e-eb7a-29f145cec8df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "Downloading https://ultralytics.com/assets/coco2017labels.zip to 'datasets/coco2017labels.zip'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46.4M/46.4M [00:00<00:00, 170MB/s]\n",
            "Unzipping datasets/coco2017labels.zip to /content/datasets/coco...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 122232/122232 [00:29<00:00, 4135.36file/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://images.cocodataset.org/zips/val2017.zip to 'datasets/coco/images/val2017.zip'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "!mkdir -p ./datasets\n",
        "\n",
        "from ultralytics.utils.downloads import download\n",
        "# Download labels\n",
        "segments = False  # segment or box labels\n",
        "dir = './datasets'\n",
        "url = 'https://github.com/ultralytics/assets/releases/download/v0.0.0/'\n",
        "urls = [url + ('coco2017labels-segments.zip' if segments else 'coco2017labels.zip')]  # labels\n",
        "download(urls, dir=dir)\n",
        "\n",
        "# Download data\n",
        "urls = 'http://images.cocodataset.org/zips/val2017.zip',  # 1G, 5k images\n",
        "download(urls, dir='./datasets/coco/images', threads=3)\n",
        "\n",
        "# !mkdir -p ../datasets\n",
        "# !wget 'https://github.com/ultralytics/assets/releases/download/v0.0.0/coco8.zip'\n",
        "# !wget 'https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/coco8.yaml'\n",
        "# !unzip coco8.zip -d ../datasets/\n",
        "# !rm coco8.zip\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "NS0yQTF2pq1Y",
        "outputId": "a122aa48-85c4-44a5-93ea-1f7d8a787fed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.35M/5.35M [00:00<00:00, 163MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.81 ğŸš€ Python-3.11.11 torch-2.5.1+cu124 CPU (Intel Xeon 2.20GHz)\n",
            "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients, 6.5 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo11n.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (5.4 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mTorchScript:\u001b[0m starting export with torch 2.5.1+cu124...\n",
            "\u001b[34m\u001b[1mTorchScript:\u001b[0m export success âœ… 8.6s, saved as 'yolo11n.torchscript' (10.5 MB)\n",
            "\n",
            "Export complete (13.6s)\n",
            "Results saved to \u001b[1m/content\u001b[0m\n",
            "Predict:         yolo predict task=detect model=yolo11n.torchscript imgsz=640  \n",
            "Validate:        yolo val task=detect model=yolo11n.torchscript imgsz=640 data=/usr/src/ultralytics/ultralytics/cfg/datasets/coco.yaml  \n",
            "Visualize:       https://netron.app\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'yolo11n.torchscript'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import glob\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision as tv\n",
        "import onnx\n",
        "import onnxruntime as ort\n",
        "from onnxruntime import quantization\n",
        "import os\n",
        "\n",
        "# for the openCV exmaple later\n",
        "import argparse\n",
        "import cv2.dnn\n",
        "from ultralytics.utils import ASSETS, yaml_load\n",
        "from ultralytics.utils.checks import check_yaml\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# download yolo11n, pretrained on coco, and export that to torchscript\n",
        "YOLO('yolo11n.pt').export(format='torchscript')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "V8ddNZvYlZHN"
      },
      "outputs": [],
      "source": [
        "# OPTIONAL - this may take a few minutes on CPU.\n",
        "# Test the .torchscript model is working correctly\n",
        "# can interupt once some images have been processed\n",
        "#!yolo val task=detect model=yolo11n.torchscript imgsz=640 data=./coco.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BThtqn4ht1iD"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "# model paths\n",
        "TORCH_SCRIPT_PATH = \"./yolo11n.torchscript\" # this should be the model downloaded from ultralytics\n",
        "ONNX_FP32_PATH = './yolo11_ts_export.onnx' # placeholder path for the converted model\n",
        "ONNX_QUANT_PREP_PATH = './yolo11_visdrone_prep.onnx' # intermediate model path\n",
        "ONNX_INT8_PATH = 'yolo11_visdrone_int8.onnx' # placeholder path for the quantised model\n",
        "\n",
        "# visdrone dataset has 10 classes, 0-9\n",
        "# annotations/ - from original dataset, all classes, in original format\n",
        "# labels/ has only 0 and 1 (pedestrian and person) classes, yolo format\n",
        "DATASET_PATH = \"/content/datasets/coco\"\n",
        "SAMPLE_IMAGE_PATH = '/content/datasets/coco/images/val2017/000000000139.jpg'\n",
        "CLASSES = yaml_load(check_yaml(\"./coco.yaml\"))[\"names\"]\n",
        "\n",
        "# (n_classes-1+5), as per yolo input format\n",
        "model_class_count = 84 # model was trained on COCO so class count needs to match shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "h3tef5vK4Mtd"
      },
      "outputs": [],
      "source": [
        "# Image transformations: Resize to n*m and normalize\n",
        "preprocess = tv.transforms.Compose([\n",
        "    tv.transforms.Resize((640, 640)),\n",
        "    tv.transforms.ToTensor(),\n",
        "    tv.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "class VisDroneValDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.img_dir = os.path.join(root_dir, \"images/val2017\")\n",
        "        self.label_dir = os.path.join(root_dir, \"labels/val2017\")\n",
        "\n",
        "        # Get all image file paths\n",
        "        self.img_paths = sorted(glob.glob(os.path.join(self.img_dir, \"*.jpg\")))\n",
        "\n",
        "        # Get all corresponding label paths\n",
        "        self.label_paths = {\n",
        "            os.path.basename(p).replace(\".txt\", \"\"): p\n",
        "            for p in glob.glob(os.path.join(self.label_dir, \"*.txt\"))\n",
        "        }\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img_paths[idx]\n",
        "        img_name = os.path.basename(img_path).replace(\".jpg\", \"\")\n",
        "\n",
        "        # Load and preprocess image\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "          # Load labels (bounding boxes)\n",
        "        label_path = self.label_paths.get(img_name, None)\n",
        "        labels = []\n",
        "        if label_path and os.path.exists(label_path):\n",
        "            with open(label_path, \"r\") as f:\n",
        "                labels = [list(map(float, line.strip().split())) for line in f.readlines()]\n",
        "\n",
        "        labels = torch.tensor(labels, dtype=torch.float32) if labels else torch.zeros((0, 5), dtype=torch.float32)\n",
        "\n",
        "\n",
        "        return image, labels  # Returns the image and its corresponding labels\n",
        "\n",
        "ds = VisDroneValDataset(DATASET_PATH, transform=preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "L3fxWEsh7_ou"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    images, labels = zip(*batch)\n",
        "\n",
        "    # Find max number of bounding boxes in a batch\n",
        "    max_boxes = max(label.shape[0] for label in labels)\n",
        "\n",
        "    # Pad labels to ensure equal shape across batch\n",
        "    padded_labels = []\n",
        "    for label in labels:\n",
        "        pad_size = max_boxes - label.shape[0]\n",
        "        if pad_size > 0:\n",
        "            pad_tensor = torch.zeros((pad_size, label.shape[1]), dtype=torch.float32)\n",
        "            padded_label = torch.cat([label, pad_tensor], dim=0)\n",
        "        else:\n",
        "            padded_label = label\n",
        "        padded_labels.append(padded_label)\n",
        "\n",
        "    return torch.stack(images), torch.stack(padded_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4MZM04rMu3R",
        "outputId": "3ef32828-1998-4a9a-f8cd-905786954234"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5000\n"
          ]
        }
      ],
      "source": [
        "# Check the size of the first image in the dataset\n",
        "image, labels = ds[0]  # Accessing the first sample\n",
        "print(len(ds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljvOrsVOumxW",
        "outputId": "e38848fd-13f4-4e56-e097-76a421af51e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from ./yolo11n.torchscript\n"
          ]
        }
      ],
      "source": [
        "calib_ds = torch.utils.data.Subset(ds, list(range(500)))  # First 50 images for calibration\n",
        "val_ds = torch.utils.data.Subset(ds, list(range(500, len(ds))))  # The rest for validation\n",
        "\n",
        "# DataLoader for validation\n",
        "dl = torch.utils.data.DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "ds = VisDroneValDataset(DATASET_PATH, transform=preprocess)\n",
        "# Load torchscript model\n",
        "model_path = TORCH_SCRIPT_PATH\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_pt = torch.jit.load(model_path, map_location=device)\n",
        "model_pt.eval()\n",
        "model_pt.to(device)\n",
        "\n",
        "model_pt.eval()\n",
        "\n",
        "print(\"Model loaded successfully from\", model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xgm6wHs1PBVo",
        "outputId": "40a0b8f4-5697-4f8a-954c-6810c4936f70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of validation dataset: 4500\n"
          ]
        }
      ],
      "source": [
        "print(f\"Size of validation dataset: {len(val_ds)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6Dnt4L7nsVo",
        "outputId": "31769352-19ef-4b75-f37d-8d39a1b9cf33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 84, 8400])\n"
          ]
        }
      ],
      "source": [
        "sample = torch.randn(1, 3, 640, 640, requires_grad=True)\n",
        "results = model_pt(sample)\n",
        "print(results.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp '/content/drive/MyDrive/uu_project/models/checkpoint0115.pth' ./"
      ],
      "metadata": {
        "id": "ZfJnOC0LtTTl"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "YHALQyXAuy-c"
      },
      "outputs": [],
      "source": [
        "# export fp32 model to onnx\n",
        "model_fp32_path = ONNX_FP32_PATH\n",
        "\n",
        "torch.onnx.export(model_pt,                                         # model\n",
        "                  sample,                                           # model input\n",
        "                  model_fp32_path,                                  # path\n",
        "                  export_params=True,                               # store the trained parameter weights inside the model file\n",
        "                  opset_version=16, #12,                                 # the ONNX version to export the model to\n",
        "                  do_constant_folding=True,                         # constant folding for optimization\n",
        "                  input_names = ['input'],                          # input names\n",
        "                  output_names = ['output'],                        # output names\n",
        "                  dynamic_axes={'input' : {0 : 'batch_size'},       # variable length axes\n",
        "                                'output' : {0 : 'batch_size'}})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "vu9kts6WjzLz"
      },
      "outputs": [],
      "source": [
        "model_fp32_path = 'model.onnx'\n",
        "model_onnx = onnx.load(model_fp32_path)\n",
        "onnx.checker.check_model(model_onnx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "SxgvIoLru3rR"
      },
      "outputs": [],
      "source": [
        "def to_numpy(tensor):\n",
        "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
        "\n",
        "class QuntizationDataReader(quantization.CalibrationDataReader):\n",
        "    def __init__(self, torch_ds, batch_size, input_name):\n",
        "        self.torch_dl = torch.utils.data.DataLoader(torch_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "        self.input_name = input_name\n",
        "        self.datasize = len(self.torch_dl)\n",
        "        self.enum_data = iter(self.torch_dl)\n",
        "\n",
        "    def to_numpy(self, pt_tensor):\n",
        "        return pt_tensor.detach().cpu().numpy() if pt_tensor.requires_grad else pt_tensor.cpu().numpy()\n",
        "\n",
        "    def get_next(self):\n",
        "        batch = next(self.enum_data, None)\n",
        "        if batch is not None:\n",
        "          return {self.input_name: self.to_numpy(batch[0])}\n",
        "        else:\n",
        "          return None\n",
        "\n",
        "    def rewind(self):\n",
        "        self.enum_data = iter(self.torch_dl)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "1T7WwxK2zAOF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dd13088-0c82-4e10-cc44-789aa2e82842"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7bebb7d5e9b0>, <onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7bebb7d5d870>]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/onnxruntime/quantization/quant_utils.py:245: RuntimeWarning: invalid value encountered in divide\n",
            "  arr_fp32 = numpy.asarray((arr.astype(numpy.float32) / scale).round() + zero_point)\n",
            "/usr/local/lib/python3.11/dist-packages/onnxruntime/quantization/quant_utils.py:247: RuntimeWarning: invalid value encountered in cast\n",
            "  return _check_type(arr_fp32.astype(dtype))\n"
          ]
        }
      ],
      "source": [
        "ort_provider = ['CPUExecutionProvider']\n",
        "if torch.cuda.is_available():\n",
        "    model_pt.to('cuda')\n",
        "    ort_provider = ['CUDAExecutionProvider']\n",
        "\n",
        "ort_sess = ort.InferenceSession(model_fp32_path, providers=ort_provider)\n",
        "\n",
        "model_prep_path = ONNX_QUANT_PREP_PATH\n",
        "quantization.shape_inference.quant_pre_process(model_fp32_path, model_prep_path, skip_symbolic_shape=True)\n",
        "qdr = QuntizationDataReader(calib_ds, batch_size=BATCH_SIZE, input_name=ort_sess.get_inputs()[0].name)\n",
        "\n",
        "q_static_opts = {\"ActivationSymmetric\":True,\n",
        "                 \"WeightSymmetric\":True}\n",
        "if torch.cuda.is_available():\n",
        "  q_static_opts = {\"ActivationSymmetric\":True,\n",
        "                  \"WeightSymmetric\":True}\n",
        "\n",
        "model_int8_path = ONNX_INT8_PATH\n",
        "quantized_model = quantization.quantize_static(model_input=model_prep_path,\n",
        "                                               model_output=model_int8_path,\n",
        "                                               calibration_data_reader=qdr,\n",
        "                                               extra_options=q_static_opts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "XqtyOUG9DmvV"
      },
      "outputs": [],
      "source": [
        "\n",
        "def draw_bounding_box(img, class_id, confidence, x, y, x_plus_w, y_plus_h):\n",
        "    \"\"\"\n",
        "    Draws bounding boxes on the input image based on the provided arguments.\n",
        "\n",
        "    Args:\n",
        "        img (numpy.ndarray): The input image to draw the bounding box on.\n",
        "        class_id (int): Class ID of the detected object.\n",
        "        confidence (float): Confidence score of the detected object.\n",
        "        x (int): X-coordinate of the top-left corner of the bounding box.\n",
        "        y (int): Y-coordinate of the top-left corner of the bounding box.\n",
        "        x_plus_w (int): X-coordinate of the bottom-right corner of the bounding box.\n",
        "        y_plus_h (int): Y-coordinate of the bottom-right corner of the bounding box.\n",
        "    \"\"\"\n",
        "    colors = np.random.uniform(0, 255, size=(len(CLASSES), 3))\n",
        "    label = f\"{CLASSES[class_id]} ({confidence:.2f})\"\n",
        "    color = colors[class_id]\n",
        "    cv2.rectangle(img, (x, y), (x_plus_w, y_plus_h), color, 2)\n",
        "    cv2.putText(img, label, (x - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "# import functions from previous notebook\n",
        "def postprocess(output, conf_threshold=0.25, iou_threshold=0.45):\n",
        "  # Reshape the output to [batch, num_anchors, (num_classes + 5)]\n",
        "  #output = output.reshape(1, 8400, model_class_count)\n",
        "  # Calculate scale factor, length / 640 (or whatever img size is defined)\n",
        "  scale = 1 # this can be one here, since length == longest side, and it's a square\n",
        "\n",
        "  # Prepare output array\n",
        "  # outputs = np.array([output[0]])\n",
        "  # Prepare output array\n",
        "  ort_outs = np.array([cv2.transpose(output[0])])\n",
        "  rows = ort_outs.shape[1]\n",
        "\n",
        "  boxes = []\n",
        "  scores = []\n",
        "  class_ids = []\n",
        "  # Iterate through output to collect bounding boxes, confidence scores, and class IDs\n",
        "  for i in range(rows):\n",
        "      classes_scores = ort_outs[0][i][4:]\n",
        "      (minScore, maxScore, minClassLoc, (x, maxClassIndex)) = cv2.minMaxLoc(classes_scores)\n",
        "      if maxScore >= 0.25:\n",
        "          box = [\n",
        "              ort_outs[0][i][0] - (0.5 * ort_outs[0][i][2]),\n",
        "              ort_outs[0][i][1] - (0.5 * ort_outs[0][i][3]),\n",
        "              ort_outs[0][i][2],\n",
        "              ort_outs[0][i][3],\n",
        "          ]\n",
        "          boxes.append(box)\n",
        "          scores.append(maxScore)\n",
        "          class_ids.append(maxClassIndex)\n",
        "\n",
        "  # Apply NMS (Non-maximum suppression)\n",
        "  ort_boxes = cv2.dnn.NMSBoxes(boxes, scores, 0.25, 0.45, 0.5)\n",
        "\n",
        "  detections = []\n",
        "  finalboxes = []\n",
        "  # Iterate through NMS results to draw bounding boxes and labels\n",
        "  for i in range(len(ort_boxes)):\n",
        "      index = ort_boxes[i]\n",
        "      box = boxes[index]\n",
        "      detection = {\n",
        "          \"class_id\": class_ids[index],\n",
        "          \"class_name\": CLASSES[class_ids[index]],\n",
        "          \"confidence\": scores[index],\n",
        "          \"box\": box,\n",
        "          \"scale\": scale,\n",
        "      }\n",
        "\n",
        "      detections.append(detection)\n",
        "      box_plot = [\n",
        "          class_ids[index],\n",
        "          scores[index],\n",
        "          round(box[0] * scale),\n",
        "          round(box[1] * scale),\n",
        "          round((box[0] + box[2]) * scale),\n",
        "          round((box[1] + box[3]) * scale),\n",
        "      ]\n",
        "      finalboxes.append(box_plot)\n",
        "  return detections, finalboxes\n",
        "\n",
        "def collate_fn(batch):\n",
        "  images, labels = zip(*batch)\n",
        "\n",
        "  # Find max number of bounding boxes in a batch\n",
        "  max_boxes = max(label.shape[0] for label in labels)\n",
        "\n",
        "  # Pad labels to ensure equal shape across batch\n",
        "  padded_labels = []\n",
        "  for label in labels:\n",
        "      pad_size = max_boxes - label.shape[0]\n",
        "      if pad_size > 0:\n",
        "          pad_tensor = torch.zeros((pad_size, label.shape[1]), dtype=torch.float32)\n",
        "          padded_label = torch.cat([label, pad_tensor], dim=0)\n",
        "      else:\n",
        "          padded_label = label\n",
        "      padded_labels.append(padded_label)\n",
        "\n",
        "  return torch.stack(images), torch.stack(padded_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "1kc9584x0Xly",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "bb82a44c-fd53-482e-9a76-e3fd4148be03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 640, 640])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'torch.Size' object has no attribute 'cpu'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-9304826d9578>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtar_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mort_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mort_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mort_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mort_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mort_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#get onnx output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-779c93016b23>\u001b[0m in \u001b[0;36mto_numpy\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mQuntizationDataReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquantization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCalibrationDataReader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'torch.Size' object has no attribute 'cpu'"
          ]
        }
      ],
      "source": [
        "ort_int8_sess = ort.InferenceSession(ONNX_INT8_PATH, providers=ort_provider)\n",
        "\n",
        "img, lab = calib_ds[0]\n",
        "img = img.reshape(1, 3, 640, 640)\n",
        "\n",
        "# Get input name for onnx model\n",
        "input_name = ort_sess.get_inputs()[0].name\n",
        "tar_size = ort_sess.get_inputs()[1].name\n",
        "print(img.shape)\n",
        "ort_inputs = {input_name: to_numpy(img), tar_size: to_numpy(img.shape)}\n",
        "ort_outs = ort_sess.run(None, ort_inputs)[0] #get onnx output\n",
        "\n",
        "#ort_int8_outs = ort_int8_sess.run(None, ort_inputs)[0]\n",
        "# print(f\"\\noutput is: {postprocess(ort_int8_outs[0])[7]}\")\n",
        "\n",
        "original_image: np.ndarray = cv2.imread(SAMPLE_IMAGE_PATH)\n",
        "[height, width, _] = original_image.shape\n",
        "\n",
        "# Prepare a square image for inference\n",
        "length = max((height, width))\n",
        "image = np.zeros((length, length, 3), np.uint8)\n",
        "image[0:height, 0:width] = original_image\n",
        "\n",
        "# Preprocess the image and prepare blob for model\n",
        "blob = cv2.dnn.blobFromImage(image, scalefactor=1 / 255, size=(640, 640), swapRB=True)\n",
        "\n",
        "input_data = blob\n",
        "\n",
        "ort_inputs = {input_name: input_data}\n",
        "ort_outs = ort_sess.run(None, ort_inputs)[0] #get onnx output\n",
        "\n",
        "bbox, final_bbox = postprocess(ort_outs)\n",
        "\n",
        "for box in final_bbox:\n",
        "    draw_bounding_box(original_image, *box)\n",
        "# Display the image with bounding boxes\n",
        "cv2.imwrite(\"./ort_int8_postprocessed.jpg\", original_image)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: the bounding boxes etc are working for this int8 onnx model, I'd like to run it on the validation data (coco) to get the results on hte full dataset\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ... (Your existing code for dataset, model loading, etc.)\n",
        "\n",
        "ort_int8_sess = ort.InferenceSession(ONNX_INT8_PATH, providers=ort_provider)\n",
        "input_name = ort_int8_sess.get_inputs()[0].name\n",
        "\n",
        "results = []\n",
        "for images, labels in tqdm(dl, desc=\"Processing validation data\"):\n",
        "    images = images.to(device)  # Move images to the correct device\n",
        "    ort_inputs = {input_name: to_numpy(images)}\n",
        "    ort_int8_outs = ort_int8_sess.run(None, ort_inputs)[0]\n",
        "\n",
        "    # batch_results = []\n",
        "    # for i in range(images.shape[0]):  # Process each image in the batch\n",
        "    #     bbox, final_bbox = postprocess(ort_int8_outs[i:i+1])\n",
        "    #     batch_results.append({\"bbox\": bbox, \"final_bbox\": final_bbox})\n",
        "    # results.append(batch_results)\n",
        "\n",
        "# Now you have the results for all validation images, organized by batch\n",
        "# Example of accessing the results:\n",
        "#\n",
        "# print(len(results))  # Number of batches\n",
        "# for batch_idx, batch_results in enumerate(results):\n",
        "#     for image_idx, image_result in enumerate(batch_results):\n",
        "#         print(f\"Batch {batch_idx}, Image {image_idx}:\")\n",
        "#         print(image_result[\"bbox\"])\n",
        "#         print(image_result[\"final_bbox\"])\n",
        "\n",
        "\n",
        "# Further processing or saving the results\n"
      ],
      "metadata": {
        "id": "OYx_-LMDnxeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = YOLO('yolo11n.pt')\n",
        "model.export(format=\"onnx\", imgsz=640)"
      ],
      "metadata": {
        "id": "uGAYs1QXcmUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo val task=detect model=yolo11n.onnx imgsz=640 data=coco128.yaml"
      ],
      "metadata": {
        "id": "osYHSDHLc0o4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo val task=detect model=yolo11n.pt imgsz=640 data=coco128.yaml"
      ],
      "metadata": {
        "id": "tTo_1NUWdHd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "!pip install onnx_tf\n",
        "from onnx_tf.backend import prepare\n",
        "\n",
        "onnx_model = onnx.load(ONNX_FP32_PATH)\n",
        "tf_rep = prepare(onnx_model)\n"
      ],
      "metadata": {
        "id": "IuwS_FhKsxd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo val"
      ],
      "metadata": {
        "id": "GIdBqjNLH67Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpMsCgVUqJxC"
      },
      "source": [
        "# OpenCV + Ultralytics Inference Example\n",
        "Only works for the FP32 ONNX format, int8 quantisation fails"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WddNWHEVjoKw"
      },
      "outputs": [],
      "source": [
        "def draw_bounding_box(img, class_id, confidence, x, y, x_plus_w, y_plus_h):\n",
        "    \"\"\"\n",
        "    Draws bounding boxes on the input image based on the provided arguments.\n",
        "\n",
        "    Args:\n",
        "        img (numpy.ndarray): The input image to draw the bounding box on.\n",
        "        class_id (int): Class ID of the detected object.\n",
        "        confidence (float): Confidence score of the detected object.\n",
        "        x (int): X-coordinate of the top-left corner of the bounding box.\n",
        "        y (int): Y-coordinate of the top-left corner of the bounding box.\n",
        "        x_plus_w (int): X-coordinate of the bottom-right corner of the bounding box.\n",
        "        y_plus_h (int): Y-coordinate of the bottom-right corner of the bounding box.\n",
        "    \"\"\"\n",
        "    colors = np.random.uniform(0, 255, size=(len(CLASSES), 3))\n",
        "    label = f\"{CLASSES[class_id]} ({confidence:.2f})\"\n",
        "    color = colors[class_id]\n",
        "    cv2.rectangle(img, (x, y), (x_plus_w, y_plus_h), color, 2)\n",
        "    cv2.putText(img, label, (x - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "\n",
        "def main(onnx_model, input_image):\n",
        "    \"\"\"\n",
        "    Main function to load ONNX model, perform inference, draw bounding boxes, and display the output image.\n",
        "\n",
        "    Args:\n",
        "        onnx_model (str): Path to the ONNX model.\n",
        "        input_image (str): Path to the input image.\n",
        "\n",
        "    Returns:\n",
        "        list: List of dictionaries containing detection information such as class_id, class_name, confidence, etc.\n",
        "    \"\"\"\n",
        "    # Load the ONNX model\n",
        "    model: cv2.dnn.Net = cv2.dnn.readNetFromONNX(onnx_model)\n",
        "\n",
        "    # Read the input image\n",
        "    original_image: np.ndarray = cv2.imread(input_image)\n",
        "    [height, width, _] = original_image.shape\n",
        "\n",
        "    # Prepare a square image for inference\n",
        "    length = max((height, width))\n",
        "    image = np.zeros((length, length, 3), np.uint8)\n",
        "    image[0:height, 0:width] = original_image\n",
        "\n",
        "    # Calculate scale factor\n",
        "    scale = length / 640\n",
        "\n",
        "    # Preprocess the image and prepare blob for model\n",
        "    blob = cv2.dnn.blobFromImage(image, scalefactor=1 / 255, size=(640, 640), swapRB=True)\n",
        "    model.setInput(blob)\n",
        "\n",
        "    # Perform inference\n",
        "    outputs = model.forward()\n",
        "\n",
        "    # Prepare output array\n",
        "    outputs = np.array([cv2.transpose(outputs[0])])\n",
        "    rows = outputs.shape[1]\n",
        "\n",
        "    boxes = []\n",
        "    scores = []\n",
        "    class_ids = []\n",
        "\n",
        "    # Iterate through output to collect bounding boxes, confidence scores, and class IDs\n",
        "    for i in range(rows):\n",
        "        classes_scores = outputs[0][i][4:]\n",
        "        (minScore, maxScore, minClassLoc, (x, maxClassIndex)) = cv2.minMaxLoc(classes_scores)\n",
        "        if maxScore >= 0.25:\n",
        "            box = [\n",
        "                outputs[0][i][0] - (0.5 * outputs[0][i][2]),\n",
        "                outputs[0][i][1] - (0.5 * outputs[0][i][3]),\n",
        "                outputs[0][i][2],\n",
        "                outputs[0][i][3],\n",
        "            ]\n",
        "            boxes.append(box)\n",
        "            scores.append(maxScore)\n",
        "            class_ids.append(maxClassIndex)\n",
        "\n",
        "    # Apply NMS (Non-maximum suppression)\n",
        "    result_boxes = cv2.dnn.NMSBoxes(boxes, scores, 0.25, 0.45, 0.5)\n",
        "\n",
        "    detections = []\n",
        "\n",
        "    # Iterate through NMS results to draw bounding boxes and labels\n",
        "    for i in range(len(result_boxes)):\n",
        "        index = result_boxes[i]\n",
        "        box = boxes[index]\n",
        "        detection = {\n",
        "            \"class_id\": class_ids[index],\n",
        "            \"class_name\": CLASSES[class_ids[index]],\n",
        "            \"confidence\": scores[index],\n",
        "            \"box\": box,\n",
        "            \"scale\": scale,\n",
        "        }\n",
        "        detections.append(detection)\n",
        "        draw_bounding_box(\n",
        "            original_image,\n",
        "            class_ids[index],\n",
        "            scores[index],\n",
        "            round(box[0] * scale),\n",
        "            round(box[1] * scale),\n",
        "            round((box[0] + box[2]) * scale),\n",
        "            round((box[1] + box[3]) * scale),\n",
        "        )\n",
        "\n",
        "    # Display the image with bounding boxes\n",
        "    cv2.imwrite(\"./image_post.jpg\", original_image)\n",
        "    cv2.waitKey(0)\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "    return detections, outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_mCvYjrhqA6"
      },
      "outputs": [],
      "source": [
        "model = ONNX_FP32_PATH\n",
        "img = SAMPLE_IMAGE_PATH\n",
        "dets, ocv_outs = main(model, img) # saves img as ./image_post.jpg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model: cv2.dnn.Net = cv2.dnn.readNetFromONNX(ONNX_FP32_PATH)\n",
        "ort_int8_sess = ort.InferenceSession(ONNX_INT8_PATH, providers=ort_provider)\n",
        "#input_data = np.random.randn(1, 3, 640, 640).astype(np.float32)\n",
        "# Read the input image\n",
        "original_image: np.ndarray = cv2.imread(SAMPLE_IMAGE_PATH)\n",
        "[height, width, _] = original_image.shape\n",
        "\n",
        "# Prepare a square image for inference\n",
        "length = max((height, width))\n",
        "image = np.zeros((length, length, 3), np.uint8)\n",
        "image[0:height, 0:width] = original_image\n",
        "\n",
        "# Preprocess the image and prepare blob for model\n",
        "blob = cv2.dnn.blobFromImage(image, scalefactor=1 / 255, size=(640, 640), swapRB=True)\n",
        "\n",
        "input_data = blob"
      ],
      "metadata": {
        "id": "LN4X9dWYMKvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get input name for onnx model\n",
        "input_name = ort_sess.get_inputs()[0].name\n",
        "ort_inputs = {input_name: blob}\n",
        "ort_outs = ort_sess.run(None, ort_inputs)[0] #get onnx output\n",
        "ort_outs = np.array([cv2.transpose(ort_outs[0])])\n",
        "rows = ort_outs.shape[1]\n",
        "\n",
        "boxes = []\n",
        "scores = []\n",
        "class_ids = []\n",
        "sclae = 1\n",
        "print(\"ORT outputs:\\n\")\n",
        "# Iterate through output to collect bounding boxes, confidence scores, and class IDs\n",
        "for i in range(rows):\n",
        "    classes_scores = ort_outs[0][i][4:]\n",
        "    (minScore, maxScore, minClassLoc, (x, maxClassIndex)) = cv2.minMaxLoc(classes_scores)\n",
        "    if maxScore >= 0.25:\n",
        "        box = [\n",
        "            ort_outs[0][i][0] - (0.5 * ort_outs[0][i][2]),\n",
        "            ort_outs[0][i][1] - (0.5 * ort_outs[0][i][3]),\n",
        "            ort_outs[0][i][2],\n",
        "            ort_outs[0][i][3],\n",
        "        ]\n",
        "        boxes.append(box)\n",
        "        scores.append(maxScore)\n",
        "        class_ids.append(maxClassIndex)\n",
        "\n",
        "# Apply NMS (Non-maximum suppression)\n",
        "ort_boxes = cv2.dnn.NMSBoxes(boxes, scores, 0.25, 0.45, 0.5)\n",
        "print(f\"\\nORT box outputs: {ort_boxes}\")\n",
        "\n",
        "detections = []\n",
        "\n",
        "# Iterate through NMS results to draw bounding boxes and labels\n",
        "for i in range(len(ort_boxes)):\n",
        "    index = ort_boxes[i]\n",
        "    box = boxes[index]\n",
        "    detection = {\n",
        "        \"class_id\": class_ids[index],\n",
        "        \"class_name\": CLASSES[class_ids[index]],\n",
        "        \"confidence\": scores[index],\n",
        "        \"box\": box,\n",
        "        \"scale\": scale,\n",
        "    }\n",
        "    detections.append(detection)\n",
        "    draw_bounding_box(\n",
        "        original_image,\n",
        "        class_ids[index],\n",
        "        scores[index],\n",
        "        round(box[0] * scale),\n",
        "        round(box[1] * scale),\n",
        "        round((box[0] + box[2]) * scale),\n",
        "        round((box[1] + box[3]) * scale),\n",
        "    )\n",
        "\n",
        "# Display the image with bounding boxes\n",
        "cv2.imwrite(\"./ort_image_post.jpg\", original_image)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "\n",
        "\n",
        "model.setInput(input_data)\n",
        "# Perform inference\n",
        "ocv_outs = model.forward()\n",
        "ocv_outs = np.array([cv2.transpose(ocv_outs[0])])\n",
        "rows = ocv_outs.shape[1]\n",
        "\n",
        "boxes = []\n",
        "scores = []\n",
        "class_ids = []\n",
        "print(\"OpenCV outputs:\\n\")\n",
        "# Iterate through output to collect bounding boxes, confidence scores, and class IDs\n",
        "for i in range(rows):\n",
        "    classes_scores = ocv_outs[0][i][4:]\n",
        "    (minScore, maxScore, minClassLoc, (x, maxClassIndex)) = cv2.minMaxLoc(classes_scores)\n",
        "    if maxScore >= 0.25:\n",
        "        box = [\n",
        "            ocv_outs[0][i][0] - (0.5 * ocv_outs[0][i][2]),\n",
        "            ocv_outs[0][i][1] - (0.5 * ocv_outs[0][i][3]),\n",
        "            ocv_outs[0][i][2],\n",
        "            ocv_outs[0][i][3],\n",
        "        ]\n",
        "        boxes.append(box)\n",
        "        scores.append(maxScore)\n",
        "        class_ids.append(maxClassIndex)\n",
        "\n",
        "# Apply NMS (Non-maximum suppression)\n",
        "ocv_boxes = cv2.dnn.NMSBoxes(boxes, scores, 0.25, 0.45, 0.5)\n",
        "print(f\"\\nOCV box outputs: {ocv_boxes}\")\n",
        "\n",
        "detections = []\n",
        "\n",
        "# Iterate through NMS results to draw bounding boxes and labels\n",
        "for i in range(len(ocv_boxes)):\n",
        "    index = ocv_boxes[i]\n",
        "    box = boxes[index]\n",
        "    detection = {\n",
        "        \"class_id\": class_ids[index],\n",
        "        \"class_name\": CLASSES[class_ids[index]],\n",
        "        \"confidence\": scores[index],\n",
        "        \"box\": box,\n",
        "        \"scale\": scale,\n",
        "    }\n",
        "    detections.append(detection)\n",
        "    draw_bounding_box(\n",
        "        original_image,\n",
        "        class_ids[index],\n",
        "        scores[index],\n",
        "        round(box[0] * scale),\n",
        "        round(box[1] * scale),\n",
        "        round((box[0] + box[2]) * scale),\n",
        "        round((box[1] + box[3]) * scale),\n",
        "    )\n",
        "\n",
        "# Display the image with bounding boxes\n",
        "cv2.imwrite(\"./ocv_image_post.jpg\", original_image)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "n2acD2tEIsGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nORT outputs:\\n{ort_outs[0]}\\nShape: {ort_outs.shape}\")\n",
        "print(f\"\\nOpenCV outputs:\\n{ocv_outs[0]}\\nShape: {ocv_outs.shape}\")"
      ],
      "metadata": {
        "id": "AJBf6rL2Lhnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15X7-S_T_78m"
      },
      "source": [
        "# Tensorflow >>> LiteRT (Tensorflow Lite)\n",
        "ultralytics method goes .pt >>> onnx >>>tflite >>> ?saved_model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ktNtZzNCZrH"
      },
      "outputs": [],
      "source": [
        "import ultralytics\n",
        "from ultralytics import YOLO\n",
        "!pip install tflite_support\n",
        "model = YOLO('yolo11n.pt')\n",
        "model.export(format=\"saved_model\", imgsz=640)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqoxz6mJFr-t"
      },
      "outputs": [],
      "source": [
        "!yolo val task=detect model=yolo11n_saved_model imgsz=640 data=./coco.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBv5RJg1ehlG"
      },
      "outputs": [],
      "source": [
        "!yolo val task=detect model=yolo11n.pt imgsz=640 data=./coco.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCey0g8QACcx"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Convert the model\n",
        "saved_model_dir = 'yolo11n_saved_model'\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir) # path to the SavedModel directory\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model.\n",
        "with open('model.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqsZoGP-JUHm"
      },
      "outputs": [],
      "source": [
        "!yolo val task=detect model=model.tflite imgsz=640 data=coco.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5B9ZBRBXKhgH"
      },
      "outputs": [],
      "source": [
        "class TFLiteModel:\n",
        "    def __init__(self, model_path: str):\n",
        "        self.interpreter = tf.lite.Interpreter(model_path)\n",
        "        self.interpreter.allocate_tensors()\n",
        "\n",
        "        self.input_details = self.interpreter.get_input_details()\n",
        "        self.output_details = self.interpreter.get_output_details()\n",
        "\n",
        "    def predict(self, *data_args):\n",
        "        assert len(data_args) == len(self.input_details)\n",
        "        for data, details in zip(data_args, self.input_details):\n",
        "            self.interpreter.set_tensor(details[\"index\"], data)\n",
        "        self.interpreter.invoke()\n",
        "        return self.interpreter.get_tensor(self.output_details[0][\"index\"])\n",
        "\n",
        "model = TFLiteModel(\"yolo11n_saved_model/yolo11n_float32.tflite\")\n",
        "input_dtype = model.input_details[0]['dtype']\n",
        "print(input_dtype)\n",
        "\n",
        "image_path = \"/content/datasets/coco/images/val2017/000000000139.jpg\"\n",
        "image = cv2.imread(image_path)\n",
        "image_resized = cv2.resize(image, (640, 640))\n",
        "image_input = image_resized.astype(np.float32)[np.newaxis]\n",
        "\n",
        "label = model.predict(image_input)\n",
        "\n",
        "# Draw bounding boxes (assuming label contains multiple detections in format [[x, y, w, h], ...])\n",
        "for bbox in label[0]:  # Access first batch if the output has batch dimension\n",
        "    bbox = np.squeeze(bbox)  # Ensure it's a 1D array\n",
        "    if len(bbox) >= 4:  # Ensure bbox has at least 4 values\n",
        "        x, y, w, h = map(int, bbox[:4])\n",
        "        cv2.rectangle(image_resized, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "\n",
        "\n",
        "# Save output image\n",
        "output_path = \"output_image.jpg\"\n",
        "cv2.imwrite(output_path, image_resized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0wqJaVjMfaA"
      },
      "outputs": [],
      "source": [
        "YOLO('yolo11n.pt').export(format=\"tflite\", imgsz=640, data='coco.yaml')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwKEj3RBNMX5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import requests\n",
        "\n",
        "from PIL import Image\n",
        "from transformers import RTDetrV2ForObjectDetection, RTDetrImageProcessor\n",
        "\n",
        "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "image_processor = RTDetrImageProcessor.from_pretrained(\"PekingU/rtdetr_v2_r18vd\")\n",
        "model = RTDetrV2ForObjectDetection.from_pretrained(\"PekingU/rtdetr_v2_r18vd\")\n",
        "\n",
        "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "     outputs = model(**inputs)\n",
        "\n",
        "results = image_processor.post_process_object_detection(outputs, target_sizes=torch.tensor([(image.height, image.width)]), threshold=0.5)\n",
        "\n",
        "for result in results:\n",
        "     for score, label_id, box in zip(result[\"scores\"], result[\"labels\"], result[\"boxes\"]):\n",
        "         score, label = score.item(), label_id.item()\n",
        "         box = [round(i, 2) for i in box.tolist()]\n",
        "         print(f\"{model.config.id2label[label]}: {score:.2f} {box}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pT9ofNCZRUy-"
      },
      "outputs": [],
      "source": [
        "!yolo val task=detect model=yolo11n.pt imgsz=640 data=coco128.yaml"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Image transformations: Resize to n*m and normalize\n",
        "preprocess = tv.transforms.Compose([\n",
        "    tv.transforms.Resize((640, 640)),\n",
        "    tv.transforms.ToTensor(),\n",
        "    #tv.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "class VisDroneValDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.img_dir = os.path.join(root_dir, \"images/train2017\")\n",
        "        self.label_dir = os.path.join(root_dir, \"labels/train2017\")\n",
        "\n",
        "        # Get all image file paths\n",
        "        self.img_paths = sorted(glob.glob(os.path.join(self.img_dir, \"*.jpg\")))\n",
        "\n",
        "        # Get all corresponding label paths\n",
        "        self.label_paths = {\n",
        "            os.path.basename(p).replace(\".txt\", \"\"): p\n",
        "            for p in glob.glob(os.path.join(self.label_dir, \"*.txt\"))\n",
        "        }\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img_paths[idx]\n",
        "        img_name = os.path.basename(img_path).replace(\".jpg\", \"\")\n",
        "\n",
        "        # Load and preprocess image\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "          # Load labels (bounding boxes)\n",
        "        label_path = self.label_paths.get(img_name, None)\n",
        "        labels = []\n",
        "        if label_path and os.path.exists(label_path):\n",
        "            with open(label_path, \"r\") as f:\n",
        "                labels = [list(map(float, line.strip().split())) for line in f.readlines()]\n",
        "\n",
        "        labels = torch.tensor(labels, dtype=torch.float32) if labels else torch.zeros((0, 5), dtype=torch.float32)\n",
        "\n",
        "\n",
        "        return image, labels  # Returns the image and its corresponding labels"
      ],
      "metadata": {
        "id": "dFxjT9L_iej4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torchvision.transforms as T\n",
        "\n",
        "\n",
        "# DataLoader for validation\n",
        "DATASET_PATH128 = '/content/datasets/coco128'\n",
        "ds = VisDroneValDataset(DATASET_PATH128, transform=preprocess)\n",
        "dl = torch.utils.data.DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "im_file = '/content/datasets/coco128/images/train2017/000000000009.jpg'\n",
        "im_pil = Image.open(im_file).convert('RGB')\n",
        "w, h = im_pil.size\n",
        "orig_size = torch.tensor([w, h])[None]\n",
        "\n",
        "transforms = T.Compose([\n",
        "    T.Resize((640, 640)),\n",
        "    T.ToTensor(),\n",
        "])\n",
        "im_data = transforms(im_pil)[None]\n",
        "\n",
        "sess = ort.InferenceSession('./model.onnx', providers=ort_provider)\n",
        "\n",
        "\n",
        "\n",
        "#ort_int8_sess = ort.InferenceSession('./model.onnx', providers=ort_provider)\n",
        "input_name = sess.get_inputs()[0].name\n",
        "\n",
        "results = []\n",
        "total_time = 0\n",
        "num_batches = 0\n",
        "\n",
        "for images, labels in tqdm(dl, desc=\"Processing coco128 train data\"):\n",
        "    images = images.to(device)  # Move images to the correct device\n",
        "    ort_inputs = {input_name: to_numpy(images)}\n",
        "\n",
        "    start_time = time.time()\n",
        "    output = sess.run(\n",
        "      output_names=None,\n",
        "      input_feed={'images': im_data.data.numpy(), \"orig_target_sizes\": orig_size.data.numpy()}\n",
        "      )\n",
        "    end_time = time.time()\n",
        "\n",
        "    inference_time = (end_time - start_time) * 1000  # Convert to ms\n",
        "    fps = 1000 / inference_time if inference_time > 0 else 0\n",
        "\n",
        "    total_time += inference_time\n",
        "    num_batches += 1\n",
        "\n",
        "    #print(f\"Inference Time: {inference_time:.2f} ms | FPS: {fps:.2f}\")\n",
        "\n",
        "avg_inference_time = total_time / num_batches if num_batches > 0 else 0\n",
        "avg_fps = 1000 / avg_inference_time if avg_inference_time > 0 else 0\n",
        "print(f\"Average Inference Time: {avg_inference_time:.2f} ms | Average FPS: {avg_fps:.2f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Os3wT199frj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "total_time = 0\n",
        "num_batches = 0\n",
        "\n",
        "for images, labels in tqdm(dl, desc=\"Processing coco128 train data\"):\n",
        "    images = images.to(device)  # Move images to the correct device\n",
        "    ort_inputs = {input_name: to_numpy(images)}\n",
        "\n",
        "    start_time = time.time()\n",
        "    ort_int8_outs = model.predict(images)\n",
        "    end_time = time.time()\n",
        "\n",
        "    inference_time = (end_time - start_time) * 1000  # Convert to ms\n",
        "    fps = 1000 / inference_time if inference_time > 0 else 0\n",
        "\n",
        "    total_time += inference_time\n",
        "    num_batches += 1\n",
        "\n",
        "    #print(f\"Inference Time: {inference_time:.2f} ms | FPS: {fps:.2f}\")\n",
        "\n",
        "avg_inference_time = total_time / num_batches if num_batches > 0 else 0\n",
        "avg_fps = 1000 / avg_inference_time if avg_inference_time > 0 else 0\n",
        "print(f\"Average Inference Time: {avg_inference_time:.2f} ms | Average FPS: {avg_fps:.2f}\")"
      ],
      "metadata": {
        "id": "yXtdK_9shrSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Copyright(c) 2023 lyuwenyu. All Rights Reserved.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import numpy as np\n",
        "import onnxruntime as ort\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "\n",
        "def draw(images, labels, boxes, scores, thrh = 0.6):\n",
        "    for i, im in enumerate(images):\n",
        "        draw = ImageDraw.Draw(im)\n",
        "\n",
        "        scr = scores[i]\n",
        "        lab = labels[i][scr > thrh]\n",
        "        box = boxes[i][scr > thrh]\n",
        "\n",
        "        for b in box:\n",
        "            draw.rectangle(list(b), outline='red',)\n",
        "            draw.text((b[0], b[1]), text=str(lab[i].item()), fill='blue', )\n",
        "\n",
        "        im.save(f'results_{i}.jpg')\n",
        "\n",
        "\n",
        "def main(args, ):\n",
        "    \"\"\"main\n",
        "    \"\"\"\n",
        "    sess = ort.InferenceSession(args.onnx_file)\n",
        "    print(ort.get_device())\n",
        "\n",
        "    im_pil = Image.open(args.im_file).convert('RGB')\n",
        "    w, h = im_pil.size\n",
        "    orig_size = torch.tensor([w, h])[None]\n",
        "\n",
        "    transforms = T.Compose([\n",
        "        T.Resize((640, 640)),\n",
        "        T.ToTensor(),\n",
        "    ])\n",
        "    im_data = transforms(im_pil)[None]\n",
        "\n",
        "    output = sess.run(\n",
        "        # output_names=['labels', 'boxes', 'scores'],\n",
        "        output_names=None,\n",
        "        input_feed={'images': im_data.data.numpy(), \"orig_target_sizes\": orig_size.data.numpy()}\n",
        "    )\n",
        "\n",
        "    labels, boxes, scores = output\n",
        "\n",
        "    draw([im_pil], labels, boxes, scores)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    import argparse\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--onnx-file', type=str, )\n",
        "    parser.add_argument('--im-file', type=str, )\n",
        "    # parser.add_argument('-d', '--device', type=str, default='cpu')\n",
        "    args = parser.parse_args()\n",
        "    main(args)\n"
      ],
      "metadata": {
        "id": "_CANHegjF40Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}